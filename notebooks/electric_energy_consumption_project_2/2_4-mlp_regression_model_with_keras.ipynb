{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># **Multi-Layer Perceptron for Regression using Keras**\n",
    ">\n",
    ">\n",
    ">The dataframe used in this notebook originates from the preprocessing steps performed in the `\"1_4b-preprocessing-feature-engineering-and-preprocessing-for-predictive-models.ipynb\"` notebook, along with some additional steps taken directly before running the Gradient Boosting Regressor (feature-target split, Winsorization to deal with outliers, logarithmic transformation of the electric range variable, and train-test split).\n",
    ">The final refinement of the selected variables is carried out here to meet the specific requirements of the models being developed, based on insights from the aforementioned notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leone\\OneDrive\\Documentos\\material_data_science_learning\\envs\\mlp_tf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "x_train, x_test, y_train, y_test = joblib.load(\"train_test_split.pkl\")\n",
    "\n",
    "print(\"Train-test split loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for Saving MLP Model and Training History\n",
    "CHECKPOINT_DIR = \"mlp_checkpoints\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.keras\")\n",
    "FINAL_MODEL_PATH = \"mlp_model_final.keras\"\n",
    "HISTORY_PATH = \"training_history.json\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m473627/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.0964 - mae: 2.4328\n",
      "Epoch 1: val_loss improved from inf to 43.70820, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1004s\u001b[0m 2ms/step - loss: 98.0953 - mae: 2.4328 - val_loss: 43.7082 - val_mae: 1.1444\n",
      "Epoch 2/100\n",
      "\u001b[1m473630/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65.3117 - mae: 2.3942\n",
      "Epoch 2: val_loss improved from 43.70820 to 37.42362, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m985s\u001b[0m 2ms/step - loss: 65.3117 - mae: 2.3942 - val_loss: 37.4236 - val_mae: 1.0258\n",
      "Epoch 3/100\n",
      "\u001b[1m473645/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62.1824 - mae: 2.3430\n",
      "Epoch 3: val_loss improved from 37.42362 to 34.93877, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1003s\u001b[0m 2ms/step - loss: 62.1823 - mae: 2.3430 - val_loss: 34.9388 - val_mae: 1.0300\n",
      "Epoch 4/100\n",
      "\u001b[1m473637/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.1582 - mae: 2.3129\n",
      "Epoch 4: val_loss improved from 34.93877 to 33.34886, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1083s\u001b[0m 2ms/step - loss: 59.1582 - mae: 2.3129 - val_loss: 33.3489 - val_mae: 1.0519\n",
      "Epoch 5/100\n",
      "\u001b[1m473630/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57.9284 - mae: 2.3140\n",
      "Epoch 5: val_loss improved from 33.34886 to 30.41097, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1115s\u001b[0m 2ms/step - loss: 57.9284 - mae: 2.3140 - val_loss: 30.4110 - val_mae: 1.0352\n",
      "Epoch 6/100\n",
      "\u001b[1m473633/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57.8955 - mae: 2.3451\n",
      "Epoch 6: val_loss did not improve from 30.41097\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1127s\u001b[0m 2ms/step - loss: 57.8955 - mae: 2.3451 - val_loss: 31.7654 - val_mae: 1.0262\n",
      "Epoch 7/100\n",
      "\u001b[1m473646/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57.7303 - mae: 2.3714\n",
      "Epoch 7: val_loss did not improve from 30.41097\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1022s\u001b[0m 2ms/step - loss: 57.7303 - mae: 2.3714 - val_loss: 30.8394 - val_mae: 0.9084\n",
      "Epoch 8/100\n",
      "\u001b[1m473645/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56.7566 - mae: 2.3756\n",
      "Epoch 8: val_loss did not improve from 30.41097\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1039s\u001b[0m 2ms/step - loss: 56.7566 - mae: 2.3756 - val_loss: 35.0901 - val_mae: 1.1497\n",
      "Epoch 9/100\n",
      "\u001b[1m473649/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56.1841 - mae: 2.3880\n",
      "Epoch 9: val_loss did not improve from 30.41097\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1081s\u001b[0m 2ms/step - loss: 56.1841 - mae: 2.3880 - val_loss: 32.8817 - val_mae: 1.0719\n",
      "Epoch 10/100\n",
      "\u001b[1m473646/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55.3038 - mae: 2.3965\n",
      "Epoch 10: val_loss did not improve from 30.41097\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1097s\u001b[0m 2ms/step - loss: 55.3038 - mae: 2.3965 - val_loss: 31.1306 - val_mae: 0.9391\n",
      "Epoch 11/100\n",
      "\u001b[1m473630/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55.0122 - mae: 2.4103\n",
      "Epoch 11: val_loss improved from 30.41097 to 29.12687, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m994s\u001b[0m 2ms/step - loss: 55.0122 - mae: 2.4103 - val_loss: 29.1269 - val_mae: 0.8800\n",
      "Epoch 12/100\n",
      "\u001b[1m473637/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53.6867 - mae: 2.4055\n",
      "Epoch 12: val_loss improved from 29.12687 to 27.24171, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m988s\u001b[0m 2ms/step - loss: 53.6867 - mae: 2.4055 - val_loss: 27.2417 - val_mae: 1.1800\n",
      "Epoch 13/100\n",
      "\u001b[1m473653/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.7749 - mae: 2.3805\n",
      "Epoch 13: val_loss did not improve from 27.24171\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1095s\u001b[0m 2ms/step - loss: 52.7749 - mae: 2.3805 - val_loss: 29.3387 - val_mae: 1.2586\n",
      "Epoch 14/100\n",
      "\u001b[1m473637/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 52.5985 - mae: 2.3733\n",
      "Epoch 14: val_loss did not improve from 27.24171\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m834s\u001b[0m 2ms/step - loss: 52.5985 - mae: 2.3733 - val_loss: 28.9501 - val_mae: 0.9518\n",
      "Epoch 15/100\n",
      "\u001b[1m473636/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 52.4005 - mae: 2.3603\n",
      "Epoch 15: val_loss improved from 27.24171 to 26.67296, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m822s\u001b[0m 2ms/step - loss: 52.4005 - mae: 2.3603 - val_loss: 26.6730 - val_mae: 0.7924\n",
      "Epoch 16/100\n",
      "\u001b[1m473641/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51.6027 - mae: 2.3564\n",
      "Epoch 16: val_loss did not improve from 26.67296\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 2ms/step - loss: 51.6027 - mae: 2.3564 - val_loss: 31.5643 - val_mae: 1.0309\n",
      "Epoch 17/100\n",
      "\u001b[1m473626/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51.8920 - mae: 2.3549\n",
      "Epoch 17: val_loss did not improve from 26.67296\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m810s\u001b[0m 2ms/step - loss: 51.8920 - mae: 2.3549 - val_loss: 28.7866 - val_mae: 0.8994\n",
      "Epoch 18/100\n",
      "\u001b[1m473622/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51.7900 - mae: 2.3512\n",
      "Epoch 18: val_loss improved from 26.67296 to 26.54928, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 2ms/step - loss: 51.7900 - mae: 2.3512 - val_loss: 26.5493 - val_mae: 1.1092\n",
      "Epoch 19/100\n",
      "\u001b[1m473636/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51.9688 - mae: 2.3531\n",
      "Epoch 19: val_loss did not improve from 26.54928\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 2ms/step - loss: 51.9688 - mae: 2.3531 - val_loss: 28.2624 - val_mae: 0.9215\n",
      "Epoch 20/100\n",
      "\u001b[1m473623/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 50.9921 - mae: 2.3357\n",
      "Epoch 20: val_loss did not improve from 26.54928\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 2ms/step - loss: 50.9921 - mae: 2.3357 - val_loss: 26.8056 - val_mae: 0.9702\n",
      "Epoch 21/100\n",
      "\u001b[1m473620/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51.3265 - mae: 2.3439\n",
      "Epoch 21: val_loss did not improve from 26.54928\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 2ms/step - loss: 51.3265 - mae: 2.3439 - val_loss: 28.5172 - val_mae: 0.8532\n",
      "Epoch 22/100\n",
      "\u001b[1m473652/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51.2040 - mae: 2.3374\n",
      "Epoch 22: val_loss improved from 26.54928 to 25.22272, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2ms/step - loss: 51.2040 - mae: 2.3374 - val_loss: 25.2227 - val_mae: 0.8494\n",
      "Epoch 23/100\n",
      "\u001b[1m473635/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51.1377 - mae: 2.3326\n",
      "Epoch 23: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 2ms/step - loss: 51.1377 - mae: 2.3326 - val_loss: 25.5743 - val_mae: 0.8622\n",
      "Epoch 24/100\n",
      "\u001b[1m473636/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50.6736 - mae: 2.3292\n",
      "Epoch 24: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 2ms/step - loss: 50.6736 - mae: 2.3292 - val_loss: 26.7885 - val_mae: 0.8896\n",
      "Epoch 25/100\n",
      "\u001b[1m473624/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51.1246 - mae: 2.3361\n",
      "Epoch 25: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m883s\u001b[0m 2ms/step - loss: 51.1246 - mae: 2.3361 - val_loss: 30.7201 - val_mae: 1.0363\n",
      "Epoch 26/100\n",
      "\u001b[1m473627/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51.2500 - mae: 2.3371\n",
      "Epoch 26: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 2ms/step - loss: 51.2500 - mae: 2.3371 - val_loss: 27.2149 - val_mae: 0.8323\n",
      "Epoch 27/100\n",
      "\u001b[1m473650/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51.3538 - mae: 2.3404\n",
      "Epoch 27: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m815s\u001b[0m 2ms/step - loss: 51.3538 - mae: 2.3404 - val_loss: 25.5450 - val_mae: 0.8009\n",
      "Epoch 28/100\n",
      "\u001b[1m473652/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50.7377 - mae: 2.3338\n",
      "Epoch 28: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m870s\u001b[0m 2ms/step - loss: 50.7377 - mae: 2.3338 - val_loss: 25.6786 - val_mae: 0.7930\n",
      "Epoch 29/100\n",
      "\u001b[1m473647/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51.3056 - mae: 2.3487\n",
      "Epoch 29: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m984s\u001b[0m 2ms/step - loss: 51.3056 - mae: 2.3487 - val_loss: 25.8586 - val_mae: 0.9487\n",
      "Epoch 30/100\n",
      "\u001b[1m473632/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.1504 - mae: 2.3424\n",
      "Epoch 30: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1365s\u001b[0m 3ms/step - loss: 51.1504 - mae: 2.3424 - val_loss: 25.4910 - val_mae: 0.9364\n",
      "Epoch 31/100\n",
      "\u001b[1m473651/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50.6308 - mae: 2.3455\n",
      "Epoch 31: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m984s\u001b[0m 2ms/step - loss: 50.6308 - mae: 2.3455 - val_loss: 25.7109 - val_mae: 0.8016\n",
      "Epoch 32/100\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50.9019 - mae: 2.3500\n",
      "Epoch 32: val_loss did not improve from 25.22272\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m899s\u001b[0m 2ms/step - loss: 50.9019 - mae: 2.3500 - val_loss: 25.3239 - val_mae: 0.9545\n",
      "\u001b[1m148017/148017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 789us/step\n",
      "MAE: 0.86\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m y_pred = model.predict(x_test_scaled).flatten()\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_absolute_error(y_test,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mR²: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_score(y_test,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Plot loss curves\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leone\\OneDrive\\Documentos\\material_data_science_learning\\envs\\mlp_tf\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:194\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m params.apply_defaults()\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leone\\OneDrive\\Documentos\\material_data_science_learning\\envs\\mlp_tf\\Lib\\inspect.py:3195\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3192\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3193\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3194\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leone\\OneDrive\\Documentos\\material_data_science_learning\\envs\\mlp_tf\\Lib\\inspect.py:3184\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3182\u001b[39m         arguments[kwargs_param.name] = kwargs\n\u001b[32m   3183\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3184\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3185\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3186\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "source": [
    "# MLP Model – Definition, Training, and Evaluation\n",
    "\n",
    "# Define a lightweight MLP\n",
    "model = Sequential([\n",
    "    Dense(64, activation = \"relu\", input_shape = (x_train_scaled.shape[1],)),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dropout(0.1),\n",
    "    Dense(1) \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\"])\n",
    "\n",
    "# Early stopping: prevent overfitting, save memory/time\n",
    "early_stop = [EarlyStopping(monitor = \"val_loss\", patience = 10, restore_best_weights = True),\n",
    "    ModelCheckpoint(\n",
    "        filepath = BEST_MODEL_PATH,\n",
    "        monitor = \"val_loss\",\n",
    "        save_best_only = True,\n",
    "        save_weights_only = False,\n",
    "        verbose = 1\n",
    "    )]\n",
    "\n",
    "# Train with smaller batch size to reduce memory load\n",
    "history = model.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 100,\n",
    "    batch_size = 32,\n",
    "    callbacks = [early_stop],\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(FINAL_MODEL_PATH)\n",
    "with open(HISTORY_PATH, \"w\") as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(x_test_scaled).flatten()\n",
    "\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"RMSE: {mean_squared_error(y_test, y_pred, squared = False):.2f}\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(history.history[\"loss\"], label = \"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"Validation Loss\")\n",
    "plt.title(\"Loss Curve (MSE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Training Interruption and Resumption\n",
    ">\n",
    "> This notebook originally contained several repeated training runs due to kernel interruptions and runtime errors (e.g., `unexpected keyword argument 'squared'`). To ensure reproducibility and clarity, all intermediate training cells have been removed.\n",
    ">\n",
    ">The training process was **interrupted after epoch 32** due to a runtime error related to an unexpected keyword argument (`squared`) in a metric or loss function. To prevent loss of progress, **checkpoints were used**, and the model was later **resumed from the last valid saved state**.\n",
    ">\n",
    ">The continuation of training — starting from **epoch 92** — is shown in the following cell. This was done by reloading the model and its training history from JSON and checkpoint files.\n",
    ">\n",
    ">All configurations for the model architecture (activation functions, dropout, optimizers, etc.) remain exactly as originally defined in the first cell. Only the training loop was resumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history from JSON file to resume training\n",
    "with open(\"training_history.json\", \"r\") as f:\n",
    "    history_data = json.load(f)\n",
    "\n",
    "last_epoch = len(history_data[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "\u001b[1m473626/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49.2141 - mae: 2.3370\n",
      "Epoch 93: val_loss improved from inf to 25.40499, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 2ms/step - loss: 49.2141 - mae: 2.3370 - val_loss: 25.4050 - val_mae: 0.7681\n",
      "Epoch 94/100\n",
      "\u001b[1m473640/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49.7111 - mae: 2.3445\n",
      "Epoch 94: val_loss improved from 25.40499 to 24.89479, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m838s\u001b[0m 2ms/step - loss: 49.7111 - mae: 2.3445 - val_loss: 24.8948 - val_mae: 1.0835\n",
      "Epoch 95/100\n",
      "\u001b[1m473618/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49.5739 - mae: 2.3419\n",
      "Epoch 95: val_loss improved from 24.89479 to 24.01681, saving model to mlp_checkpoints\\best_model.keras\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 2ms/step - loss: 49.5739 - mae: 2.3419 - val_loss: 24.0168 - val_mae: 0.7797\n",
      "Epoch 96/100\n",
      "\u001b[1m473650/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 50.2520 - mae: 2.3530\n",
      "Epoch 96: val_loss did not improve from 24.01681\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 2ms/step - loss: 50.2520 - mae: 2.3530 - val_loss: 24.3284 - val_mae: 0.9214\n",
      "Epoch 97/100\n",
      "\u001b[1m473651/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49.7181 - mae: 2.3458\n",
      "Epoch 97: val_loss did not improve from 24.01681\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m755s\u001b[0m 2ms/step - loss: 49.7181 - mae: 2.3458 - val_loss: 24.9625 - val_mae: 0.7931\n",
      "Epoch 98/100\n",
      "\u001b[1m473652/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49.8480 - mae: 2.3514\n",
      "Epoch 98: val_loss did not improve from 24.01681\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m815s\u001b[0m 2ms/step - loss: 49.8480 - mae: 2.3514 - val_loss: 25.3419 - val_mae: 0.8484\n",
      "Epoch 99/100\n",
      "\u001b[1m473631/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49.4758 - mae: 2.3416\n",
      "Epoch 99: val_loss did not improve from 24.01681\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 2ms/step - loss: 49.4758 - mae: 2.3416 - val_loss: 25.5080 - val_mae: 0.8426\n",
      "Epoch 100/100\n",
      "\u001b[1m473635/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 49.5677 - mae: 2.3445\n",
      "Epoch 100: val_loss did not improve from 24.01681\n",
      "\u001b[1m473654/473654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 2ms/step - loss: 49.5677 - mae: 2.3445 - val_loss: 26.9632 - val_mae: 0.9475\n",
      "Treinamento continuado a partir da época 92. Histórico salvo.\n"
     ]
    }
   ],
   "source": [
    "# Continue Training from Checkpoint – MLP Model\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_DIR = \"mlp_checkpoints\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.keras\")\n",
    "HISTORY_PATH = \"training_history.json\"\n",
    "\n",
    "# Reload saved model\n",
    "model = load_model(BEST_MODEL_PATH)\n",
    "\n",
    "# Load saved training history\n",
    "with open(HISTORY_PATH, \"r\") as f:\n",
    "    history_data = json.load(f)\n",
    "\n",
    "# Determine the last completed epoch\n",
    "last_epoch = len(history_data[\"loss\"])\n",
    "\n",
    "# Redefine callbacks\n",
    "early_stop = [\n",
    "    EarlyStopping(monitor = \"val_loss\", patience = 10, restore_best_weights = True),\n",
    "    ModelCheckpoint(\n",
    "        filepath = BEST_MODEL_PATH,\n",
    "        monitor = \"val_loss\",\n",
    "        save_best_only = True,\n",
    "        save_weights_only = False,\n",
    "        verbose = 1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Resume training\n",
    "history_new = model.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 100,                   \n",
    "    initial_epoch = last_epoch,    \n",
    "    batch_size = 2,\n",
    "    callbacks = early_stop,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# Update complete training history\n",
    "for key in history_new.history:\n",
    "    history_data[key].extend(history_new.history[key])\n",
    "\n",
    "with open(HISTORY_PATH, \"w\") as f:\n",
    "    json.dump(history_data, f)\n",
    "\n",
    "print(f\"Training resumed from epoch {last_epoch}. History updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m592068/592068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 662us/step\n",
      "=== Train Set ===\n",
      "R² Score: -0.0776\n",
      "MAE: 45.6552\n",
      "MSE: 3076.3042\n",
      "RMSE: 55.4644\n",
      "RMSE (% of mean): 340.48%\n",
      "Within 5% threshold? No\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model Performance on Training Set\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred_train = model.predict(x_train_scaled).flatten()\n",
    "\n",
    "# Evaluate performance on the training set\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "mean_y_train = np.mean(y_train)\n",
    "\n",
    "# Display metrics\n",
    "print(\"=== Train Set ===\")\n",
    "print(f\"R² Score: {r2_train:.4f}\")\n",
    "print(f\"MAE: {mae_train:.4f}\")\n",
    "print(f\"MSE: {mse_train:.4f}\")\n",
    "print(f\"RMSE: {rmse_train:.4f}\")\n",
    "print(f\"RMSE (% of mean): {100 * rmse_train / mean_y_train:.2f}%\")\n",
    "print(f\"Within 5% threshold? {'Yes' if rmse_train / mean_y_train < 0.05 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148017/148017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 722us/step\n",
      "R² Score: -0.0779\n",
      "MAE: 45.6559\n",
      "MSE: 3077.1706\n",
      "RMSE: 55.4723\n",
      "RMSE (% of mean): 340.45%\n",
      "Within 5% threshold? No\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model Performance on Test Set\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(x_test_scaled).flatten()\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mean_y = np.mean(y_test)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"RMSE (% of mean): {100 * rmse / mean_y:.2f}%\")\n",
    "print(f\"Within 5% threshold? {'Yes' if rmse / mean_y < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Results\n",
    "\n",
    "**Train Set Evaluation:**  \n",
    "- **R² Score:** -0.0776  \n",
    "- **Mean Absolute Error:** 45.6552  \n",
    "- **Mean Squared Error:** 3076.3042  \n",
    "- **Root Mean Squared Error:** 55.4644  \n",
    "- **RMSE as % of mean:** 340.48%  \n",
    "- **Within 5% threshold?** No\n",
    "\n",
    "**Test Set Evaluation:**  \n",
    "- **R² Score:** -0.0779  \n",
    "- **Mean Absolute Error:** 45.6559  \n",
    "- **Mean Squared Error:** 3077.1706  \n",
    "- **Root Mean Squared Error:** 55.4723  \n",
    "- **RMSE as % of mean:** 340.45%  \n",
    "- **Within 5% threshold?** No\n",
    "\n",
    "**Additional Insight:**  \n",
    "- **Mean Electric Energy Consumption (excluding zeros):** 182.3966  \n",
    "\n",
    "The deep learning model, despite its theoretical capacity to capture complex patterns, **failed to produce meaningful results** in this phase. The R² values are negative for both training and test sets, indicating that the model performs **worse than a constant prediction of the mean**. The RMSE exceeds 340% of the target mean, which points to **large deviations** between predictions and actual values.\n",
    "\n",
    "Moreover, the MAE values are high and practically identical across both sets, suggesting that the model is not overfitting — it simply **did not learn relevant patterns from the data**.\n",
    "\n",
    "These results suggest that either the model architecture is unsuitable, the data preprocessing was not compatible with deep learning requirements, or the model was **not adequately trained or optimized** for this task. Further work would be required to investigate:\n",
    "\n",
    "- Network depth and layer design  \n",
    "- Normalization and feature scaling strategies  \n",
    "- Loss function suitability and optimizer settings  \n",
    "- Number of training epochs and batch size  \n",
    "- Volume and structure of the training data\n",
    "\n",
    "At this stage, however, the deep learning model serves more as a **control case** than a viable solution. It underscores the importance of careful tuning and preprocessing when working with neural networks and highlights the robustness of tree-based models like HistGradientBoosting and LightGBM for this particular dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
